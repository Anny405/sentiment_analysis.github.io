<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Conversations Are Long, Emotions Are Subtle</title>
  <!-- external -->
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- HERO -->
  <header class="hero">
    <div class="hero-inner">
      <div class="hero-text">
        <h1 class="hero-title">
          Conversations Are Long, Emotions Are Subtle
        </h1>
        <h2 class="hero-subtitle">
          How Memory Networks, Transformer-XL, and Emotion Anchors Reshape Conversational Emotion Recognition
        </h2>
        <div class="hero-meta">
          DS-GA 1011 · Fall 2025 · Ellie Wang, Georgios Ioannou, Qiya Huang, Ruokai Gu, Ziyu Qi
        </div>
      </div>
      <div class="hero-image">
        <img src="images/hybrid-flowchart.svg" alt="Hybrid model flowchart" class="hero-flow">
      </div>
    </div>
  </header>

  <!--body -->
  <div class="page">
    <main class="content">
      <h2>Why conversational emotion is hard</h2>
      <p>
        Single-utterance emotion classification is already challenging, but conversational emotion recognition raises the difficulty in several important ways. Emotions in dialogue rarely appear in clean, self-contained sentences. A single line like <i>“It’s fine, whatever”</i> might signal annoyance, resignation, or genuine agreement—its meaning depends almost entirely on the surrounding context and the speaker’s intent.
      </p>      
      <p>
        More importantly, emotions in conversation unfold over time. They accumulate across turns, shift gradually, and often blur the boundaries between categories such as frustration, annoyance, and sadness. Yet many existing systems still analyze each utterance in isolation, overlooking the long-range dependencies and speaker-specific patterns that shape how emotions evolve within a dialogue.
      </p>      
      <p>
        This blog explores whether modern neural architectures can better follow these emotional trajectories. Rather than proposing a new architecture from scratch, we revisit three influential ideas in conversational emotion recognition—speaker-specific memory networks (CMN), long-context transformers (Transformer-XL), and emotion-anchored contrastive learning (EACL). Each addresses a different limitation in how models interpret emotional dynamics.
      </p>
      <p>
        We reproduce all three models on the IEMOCAP benchmark and study where they excel and where they fall short, centered around a guiding question:
        <strong>Can long-range context modeling and structured emotional representations be combined into a single, more interpretable system?</strong>
      </p>
      <p>
        The sections that follow trace this investigation—from CMN’s limited short-term memory, to Transformer-XL’s ability to capture emotional buildup across long conversations, to EACL’s reshaping of the embedding space using learned emotion anchors. Along the way, we compare their behaviors side-by-side, highlight a few surprising patterns, and outline a hybrid approach that integrates their most promising ideas.
      </p>







      <h2 id="eacl">Emotion-Anchored Contrastive Learning (EACL)</h2>
      
      <p>
        Emotion-Anchored Contrastive Learning reframes emotion classification as a 
        <strong>geometric learning problem</strong>. Instead of relying on the encoder 
        to implicitly discover class boundaries, EACL introduces a set of 
        <strong>trainable emotion anchors</strong>—one prototype vector per emotion category.
        During training, each utterance embedding is pulled toward its corresponding anchor 
        and pushed away from others, gradually reshaping the geometry of the embedding space.
      </p>
      
      <p>
        Before training, embeddings often occupy overlapping regions:
        <i>frustrated</i> lies between <i>angry</i> and <i>sad</i>,
        and <i>happy</i> drifts toward <i>excited</i>. These overlaps reflect the ambiguity
        of conversational emotions but also create unstable class boundaries.  
        After EACL training, clusters tighten around anchors and the space becomes more structured,
        even for borderline categories.
      </p>
      
      <div class="figure">
        <img src="images/eacl-anchor.png" 
          alt="Emotion anchors and embedding geometry" 
          class="figure-img figure-img-png svg-isolated">
        <p class="figure-caption">
          <strong>Figure.</strong> Utterance embeddings (dots) and learned emotion anchors (stars).
          Training organizes the space around anchors, separating borderline categories.
        </p>
      </div>
      
      <p>
        The impact of anchors becomes more evident in pairwise similarity and angular-distance
        visualizations. Before training, similarity matrices show diffuse patterns—emotion pairs 
        share unexpectedly high similarity, and negative emotions lack a stable geometric relationship.
        After training, diagonals strengthen while off-diagonal values decrease, indicating clearer boundaries.
      </p>
      
      <div class="figure">
        <img src="images/EACL-heatmap.png" 
          alt="Similarity and angle heatmaps before and after EACL training" 
          class="figure-img figure-img-png svg-isolated">
        <p class="figure-caption">
          <strong>Figure.</strong> Similarity and angular distances before and after applying EACL.
          Representations become more consistent and class boundaries sharpen significantly.
        </p>
      </div>
      
      <p>
        Taken together, these patterns highlight EACL’s key contribution:
        <strong>it provides a geometric prior for emotional organization</strong>.
        Anchors stabilize the representation space while preserving contextual nuance, creating
        cleaner and more interpretable decision boundaries—an especially valuable property for 
        conversational emotion modeling.
      </p>



      <section id="ket">
  <h2>Knowledge-Enriched Transformer (KET)</h2>
  <p>
    While CMN, Transformer-XL, and EACL all operate purely on text, real conversations
    often express emotions indirectly through events and situations rather than explicit
    sentiment words. KET is our attempt to address this gap by bringing external
    knowledge into the model.
  </p>
  <div class="figure">
  <img src="images/KET_architecture.png"
       alt="Knowledge-Enriched Transformer architecture"
       class="figure-img figure-img-png svg-isolated">

  <p class="figure-caption">
    <strong>Figure 3.</strong>
    Architecture of the Knowledge-Enriched Transformer (KET).  
    The model integrates word embeddings with concept representations obtained from an external
    knowledge base (ConceptNet + NRC-VAD). A dynamic affective graph attention layer fuses these
    signals before feeding them into multi-head self-attention and cross-attention modules.
  </p>
</div>
  <h3>What KET Adds</h3>
  <ul>
    <li><strong>Commonsense knowledge (ConceptNet):</strong> connects words to the kinds
      of events and situations people usually associate with emotions
      (e.g., <em>funeral → sadness</em>, <em>celebration → joy</em>).</li>
    <li><strong>Affective signals (NRC-VAD):</strong> gives each word continuous scores
      for valence, arousal, and dominance, so the model has a sense of how positive,
      intense, or controlling a word feels.</li>
    <li><strong>Knowledge-enriched representations:</strong> these signals are injected
      into the token embeddings before the transformer layers, so attention is computed
      over a space that already encodes emotional structure, not just raw text.</li>
  </ul>

  <h3>Why It Helps</h3>
  <ul>
    <li>Makes it easier to recognize emotions that are implied rather than stated
      directly.</li>
    <li>Improves performance on subtle or minority emotion classes, such as
      <em>fear</em> and <em>disgust</em>.</li>
    <li>Acts as an add-on module that can sit on top of CMN, Transformer-XL, or EACL,
      rather than replacing them.</li>
  </ul>
</section>

      
      <h2 id="new-insights">New Insights: Beyond Individual Architectures</h2>
      <p>
        Looking across CMN, Transformer-XL, and EACL, a consistent pattern emerges: 
        conversational emotion recognition cannot be improved by treating context modeling 
        and representation learning as separate problems. Each model addresses a different 
        weakness, but their failure modes tend to cluster around the same “borderline” emotions, 
        such as <i>frustration</i> versus <i>anger</i> or <i>excited</i> versus <i>happy</i>.
      </p>
      
      <p>
        CMN’s speaker-specific memory helps with short-range dependencies and identity cues, 
        but its fixed window struggles once emotions build up slowly over many turns. 
        Transformer-XL extends the effective context, capturing gradual emotional drift across a dialogue, 
        yet its embedding space remains relatively unstructured, leading to confusion 
        near fuzzy class boundaries. EACL, on the other hand, imposes a clean, 
        anchor-based geometry on the emotion space, but operates with more limited context 
        and has less direct access to long-range conversational dynamics.
      </p>
      
      <!-- FIGURE 1 -->
      <div class="figure">
        <img src="images/newinsight_flowchart.svg" 
             alt="Complementary capabilities of CMN, Transformer-XL, and EACL"
             class="figure-img svg-isolated">
        <p class="figure-caption">
          <strong>Figure 1.</strong> 
          Each architecture contributes a distinct capability—speaker memory, long-range recurrence, 
          and structured emotion space—that naturally converges toward a hybrid design.
        </p>
      </div>
      
      <p>
        Taken together, these observations suggest a broader perspective: 
        <strong>emotion in conversation is simultaneously a temporal and a geometric phenomenon.</strong>
        Models need both a mechanism to follow how emotions evolve over time 
        and a representation space where related emotions remain close but still separable. 
        This view motivates the hybrid architecture proposed later in the post, 
        which combines Transformer-XL–style long-context recurrence with EACL’s emotion anchors 
        to unify temporal stability with clearer emotional boundaries.
      </p>
      
      <!-- FIGURE 2 -->
      <div class="figure">
        <img src="images/newinsight_compare.svg" 
             alt="Embedding geometry" 
             class="figure-img svg-isolated">

        <p class="figure-caption">
          <strong>Figure 2.</strong> 
          Emotion anchors reshape the embedding geometry, separating borderline categories 
          such as <i>anger</i> vs. <i>frustration</i> and producing more stable class boundaries.
        </p>
      </div>
    

<section id="key-insights">
  <h2>Key Insights</h2>
  <ol>
    <li><strong>Different models capture different aspects of emotional context.</strong>
      CMN focuses on speaker memory, Transformer-XL on long-range context, and EACL on
      cleaner emotion separation. Each one fixes a different weakness.</li>
    <li><strong>Combining ideas is more powerful than relying on a single architecture.</strong>
      Our comparisons and hybrid experiments suggest that memory, long context, and
      structured emotion representations complement each other rather than competing.</li>
    <li><strong>External knowledge fills a gap that text alone cannot cover.</strong>
      KET shows that adding commonsense and affective information helps the model
      interpret implicit or ambiguous emotions, especially for the harder classes.</li>
  </ol>
</section>



  </main>
    <aside class="sidebar">
      <div class="sidebar-title">QUICK LINKS</div>
      <ul class="sidebar-link-list">
        <li><a href="https://github.com/Anny405/ziyuqi.github.io" target="_blank">GitHub Repo</a></li>
        <li><a href="POSTER_PDF_LINK_HERE" target="_blank">Poster (PDF)</a></li>
        <li><a href="#hybrid">Hybrid Model</a></li>
      </ul>

      <div class="sidebar-title">SHARE</div>
      <p>
        <a href="https://anny405.github.io/ziyuqi.github.io/" target="_blank">
          Share this page
        </a>
      </p>
    </aside>
  </div>

</body>
</html>

