<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Conversations Are Long, Emotions Are Subtle</title>
  <!-- external -->
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- HERO -->
  <header class="hero">
    <div class="hero-inner">
      <div class="hero-text">
        <h1 class="hero-title">
          Conversations Are Long, Emotions Are Subtle
        </h1>
        <h2 class="hero-subtitle">
          How Memory Networks, Transformer-XL, and Emotion Anchors Reshape Conversational Emotion Recognition
        </h2>
        <div class="hero-meta">
          DS-GA 1011 · Fall 2025 · Ellie Wang, Georgios Ioannou, Qiya Huang, Ruokai Gu, Ziyu Qi
        </div>
      </div>
      <div class="hero-image">
        <img src="images/hybrid-flowchart.svg" alt="Hybrid model flowchart" class="hero-flow">
      </div>
    </div>
  </header>

  <!--body -->
  <div class="page">
    <main class="content">
      <h2>Why conversational emotion is hard</h2>
      <p>
        Single-utterance emotion classification is already challenging, but conversational emotion recognition raises the difficulty in several important ways. Emotions in dialogue rarely appear in clean, self-contained sentences. A single line like <i>“It’s fine, whatever”</i> might signal annoyance, resignation, or genuine agreement—its meaning depends almost entirely on the surrounding context and the speaker’s intent.
      </p>      
      <p>
        More importantly, emotions in conversation unfold over time. They accumulate across turns, shift gradually, and often blur the boundaries between categories such as frustration, annoyance, and sadness. Yet many existing systems still analyze each utterance in isolation, overlooking the long-range dependencies and speaker-specific patterns that shape how emotions evolve within a dialogue.
      </p>      
      <p>
        This blog explores whether modern neural architectures can better follow these emotional trajectories. Rather than proposing a new architecture from scratch, we revisit three influential ideas in conversational emotion recognition—speaker-specific memory networks (CMN), long-context transformers (Transformer-XL), and emotion-anchored contrastive learning (EACL). Each addresses a different limitation in how models interpret emotional dynamics.
      </p>
      <p>
        We reproduce all three models on the IEMOCAP benchmark and study where they excel and where they fall short, centered around a guiding question:
        <strong>Can long-range context modeling and structured emotional representations be combined into a single, more interpretable system?</strong>
      </p>
      <p>
        The sections that follow trace this investigation—from CMN’s limited short-term memory, to Transformer-XL’s ability to capture emotional buildup across long conversations, to EACL’s reshaping of the embedding space using learned emotion anchors. Along the way, we compare their behaviors side-by-side, highlight a few surprising patterns, and outline a hybrid approach that integrates their most promising ideas.
      </p>


      
      <h2 id="new-insights">New Insights: Beyond Individual Architectures</h2>
      <p>
        Looking across CMN, Transformer-XL, and EACL, a consistent pattern emerges: 
        conversational emotion recognition cannot be improved by treating context modeling 
        and representation learning as separate problems. Each model addresses a different 
        weakness, but their failure modes tend to cluster around the same “borderline” emotions, 
        such as <i>frustration</i> versus <i>anger</i> or <i>excited</i> versus <i>happy</i>.
      </p>
      
      <p>
        CMN’s speaker-specific memory helps with short-range dependencies and identity cues, 
        but its fixed window struggles once emotions build up slowly over many turns. 
        Transformer-XL extends the effective context, capturing gradual emotional drift across a dialogue, 
        yet its embedding space remains relatively unstructured, leading to confusion 
        near fuzzy class boundaries. EACL, on the other hand, imposes a clean, 
        anchor-based geometry on the emotion space, but operates with more limited context 
        and has less direct access to long-range conversational dynamics.
      </p>
      
      <!-- FIGURE 1 -->
      <div class="figure">
        <img src="images/newinsight_flowchart.svg" 
             alt="Complementary capabilities of CMN, Transformer-XL, and EACL"
             class="figure-img svg-isolated">
        <p class="figure-caption">
          <strong>Figure 1.</strong> 
          Each architecture contributes a distinct capability—speaker memory, long-range recurrence, 
          and structured emotion space—that naturally converges toward a hybrid design.
        </p>
      </div>
      
      <p>
        Taken together, these observations suggest a broader perspective: 
        <strong>emotion in conversation is simultaneously a temporal and a geometric phenomenon.</strong>
        Models need both a mechanism to follow how emotions evolve over time 
        and a representation space where related emotions remain close but still separable. 
        This view motivates the hybrid architecture proposed later in the post, 
        which combines Transformer-XL–style long-context recurrence with EACL’s emotion anchors 
        to unify temporal stability with clearer emotional boundaries.
      </p>
      
      <!-- FIGURE 2 -->
      <div class="figure">
        <img src="images/newinsight_compare.svg" 
             alt="Embedding geometry" 
             class="figure-img svg-isolated">

        <p class="figure-caption">
          <strong>Figure 2.</strong> 
          Emotion anchors reshape the embedding geometry, separating borderline categories 
          such as <i>anger</i> vs. <i>frustration</i> and producing more stable class boundaries.
        </p>
      </div>





  </main>
    <aside class="sidebar">
      <div class="sidebar-title">QUICK LINKS</div>
      <ul class="sidebar-link-list">
        <li><a href="https://github.com/Anny405/ziyuqi.github.io" target="_blank">GitHub Repo</a></li>
        <li><a href="POSTER_PDF_LINK_HERE" target="_blank">Poster (PDF)</a></li>
        <li><a href="#hybrid">Hybrid Model</a></li>
        <li><a href="#experiments">Experiments</a></li>
      </ul>

      <div class="sidebar-title">SHARE</div>
      <p>
        Copy and share this link:<br>
        <code>https://anny405.github.io/ziyuqi.github.io/</code>
      </p>
    </aside>
  </div>

</body>
</html>

