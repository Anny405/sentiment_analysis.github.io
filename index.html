<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Conversations Are Long, Emotions Are Subtle</title>
  <!-- external -->
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- HERO -->
  <header class="hero">
    <div class="hero-inner">
      <div class="hero-text">
        <h1 class="hero-title">
          Conversations Are Long, Emotions Are Subtle
        </h1>
        <h2 class="hero-subtitle">
          How Memory Networks, Transformer-XL, and Emotion Anchors Reshape Conversational Emotion Recognition
        </h2>
        <div class="hero-meta">
          DS-GA 1011 · Fall 2025 · Ellie Wang, Georgios Ioannou, Qiya Huang, Ruokai Gu, Ziyu Qi
        </div>
      </div>
      <div class="hero-image">
        <img src="images/hybrid-flowchart.svg" alt="Hybrid model flowchart" class="hero-flow">
      </div>
    </div>
  </header>

  <!--body -->
  <div class="page">
    <main class="content">
      <!-- <h2 id="intro">1. Introduction</h2>
      <p>
        Conversational emotion recognition goes far beyond assigning a label to a single sentence.
        In real dialogue, emotions build up over many turns, depend on who is speaking, and often
        blur the boundary between categories like <i>angry</i> and <i>frustrated</i>.
        In this post, we revisit three influential ideas—speaker-specific memory networks,
        long-context transformers, and emotion-anchored contrastive learning—and ask a simple question:
        how can we combine them to better track subtle emotional dynamics in conversation?
      </p> -->
      <p>
        Emotions in conversation rarely surface all at once. They build up over multiple turns, shift with speaker intent, and often blur the boundary between categories like frustration, annoyance, and sadness. Yet many emotion recognition systems still treat each utterance in isolation, largely ignoring the long-range dependencies and speaker-specific cues that shape how emotions emerge in dialogue.
      </p>
      <p>
        In this work, we ask whether modern neural architectures can follow these emotional trajectories more faithfully over time. Instead of introducing a completely new model, we revisit three influential ideas in conversational emotion recognition—speaker-specific memory networks (CMN), long-context transformers (Transformer-XL), and emotion-anchored contrastive learning (EACL). Each targets a different weakness in how current systems model emotional dynamics.
      </p>
      <p>
        We reproduce all three models on the IEMOCAP benchmark and examine where they succeed and where they break down, with a central question in mind:
        <strong>Can long-range context modeling and structured emotion representations be combined into a single, more interpretable architecture?</strong>
      </p>
      <p>
        The rest of this blog traces that question: from the limitations of CMN’s short-term memory, to Transformer-XL’s ability to track emotional build-up, to EACL’s reshaping of the embedding space via learned emotion anchors. Along the way, we contrast their behaviors, highlight a few unexpected patterns, and outline a hybrid design that merges the most effective ideas.
      </p>

      <h2 id="problem">2. Problem Setup: Conversational Emotion Recognition</h2>
      <p>
        We focus on utterance-level emotion recognition in multi-turn dialogues.
        Each conversation is a sequence of utterances produced by different speakers, and the goal is
        to predict an emotion label (e.g., <i>happy</i>, <i>sad</i>, <i>angry</i>) for every utterance.
        Our main benchmark is the IEMOCAP dataset, a widely used corpus of dyadic conversations with
        speaker-annotated emotion labels.
      </p>

      <h2 id="cmn">3. Conversational Memory Networks (CMN, 2018)</h2>
      <p>
        CMN introduces speaker-specific memory modules that store recent utterances for each participant.
        Multimodal features from text, audio, and visual channels are fused into a single vector and written
        into the corresponding speaker memory. An attention mechanism then reads from these memories to
        produce a context-aware representation for the current utterance, which is fed into a classifier.
        CMN captures short-term emotional dependencies and speaker identity, but its fixed-size memory window
        limits its ability to model long conversations and gradual emotional shifts.
      </p>

      <h2 id="txl">4. Transformer-XL for Long-Context Modeling</h2>
      <p>
        Standard transformers operate on fixed-length segments, which fragments long dialogues and prevents
        the model from reusing information across segments. Transformer-XL addresses this by introducing
        segment-level recurrence: hidden states from previous segments are cached and reused when encoding
        new ones. Relative positional encodings further resolve confusion about where an utterance appears
        in the overall sequence. When applied to conversational emotion recognition, Transformer-XL improves
        the model's ability to track how emotions build up over many turns.
      </p>

      <h2 id="eacl">5. Emotion-Anchored Contrastive Learning (EACL, 2024)</h2>
      <p>
        Even with a strong encoder, many emotion categories remain confusable in representation space.
        EACL proposes to learn a set of trainable <i>emotion anchors</i>, one prototype vector per class.
        During training, each utterance embedding is pulled toward its corresponding anchor and pushed away
        from others via a contrastive loss, while an additional anchor-angle loss keeps anchors themselves
        well separated. This reshapes the embedding space so that similar emotions remain close but class
        boundaries become clearer.
      </p>

      <h2 id="hybrid">6. Hybrid Architecture: Long Context + Emotion Anchors</h2>
      <p>
        Our main insight is that long-context modeling and emotion-anchored representation learning are
        complementary. We therefore extend the CMN baseline by replacing its utterance encoder with a
        Transformer-XL encoder and attaching an EACL-style contrastive head on top of the encoder outputs.
        The overall loss combines cross-entropy for label prediction, a contrastive term for utterance–anchor
        alignment, and an anchor-angle term that encourages diverse emotion prototypes. This hybrid design
        is intended to promote both context stability across long conversations and clearer emotional
        boundaries in the embedding space.
      </p>

      <h2 id="experiments">7. Experiments and Comparative Analysis</h2>
      <p>
        In our study, we reproduce CMN, Transformer-XL, and EACL on the IEMOCAP dataset and compare their
        performance under a unified evaluation setup. We also analyze confusion matrices and embedding
        visualizations to understand how each architecture handles similar emotion categories such as
        <i>angry</i> vs. <i>frustrated</i>. Finally, we outline an evaluation protocol for the proposed
        hybrid model, including metrics, cross-dataset tests, and speaker-independent splits.
      </p>

      <h2 id="conclusion">8. Discussion and Conclusion</h2>
      <p>
        Conversational emotion recognition challenges models to reason over long context, speaker identity,
        and subtle differences between related emotions. Memory networks, recurrent transformers, and
        contrastive emotion anchors each address a different part of this problem. By viewing them together
        and proposing a hybrid architecture, we highlight how future work can move beyond single-model
        improvements toward more integrated and interpretable approaches to emotion in dialogue.
      </p>
    </main>

    <aside class="sidebar">
      <div class="sidebar-title">QUICK LINKS</div>
      <ul class="sidebar-link-list">
        <li><a href="https://github.com/Anny405/ziyuqi.github.io" target="_blank">GitHub Repo</a></li>
        <li><a href="POSTER_PDF_LINK_HERE" target="_blank">Poster (PDF)</a></li>
        <li><a href="#hybrid">Hybrid Model</a></li>
        <li><a href="#experiments">Experiments</a></li>
      </ul>

      <div class="sidebar-title">SHARE</div>
      <p>
        Copy and share this link:<br>
        <code>https://anny405.github.io/ziyuqi.github.io/</code>
      </p>
    </aside>
  </div>

</body>
</html>

